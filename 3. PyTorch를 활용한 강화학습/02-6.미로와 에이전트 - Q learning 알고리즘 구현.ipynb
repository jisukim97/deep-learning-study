{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미로 구현\n",
    "초기상태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = plt.gca()\n",
    "\n",
    "#붉은 벽 그리기\n",
    "plt.plot([1,1],[0,1], color='red', linewidth=2)\n",
    "plt.plot([1,2],[2,2], color='red', linewidth=2)\n",
    "plt.plot([2,2],[2,1], color='red', linewidth=2)\n",
    "plt.plot([2,3],[1,1], color='red', linewidth=2)\n",
    "\n",
    "#상태 문자열 표시\n",
    "plt.text(0.5, 2.5, 'S0', size=14, ha = 'center')\n",
    "plt.text(1.5, 2.5, 'S1', size=14, ha = 'center')\n",
    "plt.text(2.5, 2.5, 'S2', size=14, ha = 'center')\n",
    "plt.text(0.5, 1.5, 'S3', size=14, ha = 'center')\n",
    "plt.text(1.5, 1.5, 'S4', size=14, ha = 'center')\n",
    "plt.text(2.5, 1.5, 'S5', size=14, ha = 'center')\n",
    "plt.text(0.5, 0.5, 'S6', size=14, ha = 'center')\n",
    "plt.text(1.5, 0.5, 'S7', size=14, ha = 'center')\n",
    "plt.text(2.5, 0.5, 'S8', size=14, ha = 'center')\n",
    "\n",
    "# x,y축 범위 설정 및 눈금 제거\n",
    "ax.set_xlim(0,3)\n",
    "ax.set_ylim(0,3)\n",
    "plt.tick_params(axis='both', which='both',bottom=False, top=False, labelbottom=False, right=False, left=False, labelleft=False)\n",
    "\n",
    "#시작점 표시\n",
    "line, = ax.plot([0.5],[2.5], marker='o', color='g', markersize=55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 에이전트 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정책을 결정하는 파라미터의 초깃값 theta_0을 설정\n",
    "#row - 상태0~7, col - 행동(상,우,하,좌)\n",
    "\n",
    "theta_0 = np.array([[np.nan, 1, 1, np.nan], #S0일 때 가능한 행동이면 1, 가능하지 않은 방향이면 nan\n",
    "                     [np.nan, 1, np.nan, 1], #S1\n",
    "                     [np.nan, np.nan, 1, 1], #S2\n",
    "                     [1, 1, 1 , np.nan], #S3\n",
    "                     [np.nan, np.nan, 1, 1], #4\n",
    "                     [1, np.nan, np.nan, np.nan], #S5\n",
    "                     [1, np.nan, np.nan, np.nan], #S6\n",
    "                     [1, 1, np.nan, np.nan] #S7\n",
    "                    ]) #S8은 목표지점이기 때문에 정책 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ϵ-greedy algorithm으로 정책 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행동가치 함수 Q 행 - 상태 s, 열은 행동 a : Q(s,a)\n",
    "# Q의 초기상태\n",
    "\n",
    "[a,b] = theta_0.shape #열과 행의 개수를 변수 a,b에 저장\n",
    "Q = np.random.rand(a,b) * theta_0\n",
    "# * theta0으로 요소단위 곱셈을 수행, Q에서 벽방향으로 이동하는 행동에는 nan부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정책 파라미터 theta_0을 무작위 행동 정책 pi로 변환하는 함수\n",
    "#epsilon으로 무작위 행동 취하고 나머지 확률로 1-epsilon은 Q값이 큰 행동을 취하도록 하고 epsilon은 점점 줄어들게하여\n",
    "#explore-exploit trade off를 적절히 섞도록 한다\n",
    "\n",
    "def simple_convert_into_pi_from_theta(theta):\n",
    "    [m,n] = theta.shape\n",
    "    pi = np.zeros((m,n))\n",
    "    for i in range(0,m):\n",
    "        pi[i,:] = theta[i,:] / np.nansum(theta[i,:]) #단순 비율 계산\n",
    "    \n",
    "    pi = np.nan_to_num(pi)\n",
    "    return pi\n",
    "\n",
    "#무작위 행동정책 pi_0을 계산\n",
    "pi_0 = simple_convert_into_pi_from_theta(theta_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ϵ-greedy 알고리즘 \n",
    "\n",
    "#행동 결정\n",
    "def get_action(s, Q, epsilon, pi_0):\n",
    "    direction = ['up','right','down','left']\n",
    "    \n",
    "    #행동 결정\n",
    "    if np.random.rand() < epsilon: #입실론 보다 작으면 랜덤 행동\n",
    "        next_direction = np.random.choice(direction, p=pi_0[s,:])\n",
    "        \n",
    "    else: #그렇지 않으면 Q값이 최대가 되는 행동\n",
    "        next_direction = direction[np.nanargmax(Q[s,:])]\n",
    "        \n",
    "    #행동을 인덱스로 변환\n",
    "    if next_direction == 'up':\n",
    "        action = 0\n",
    "    elif next_direction == 'right':\n",
    "        action = 1\n",
    "    elif next_direction == 'down':\n",
    "        action = 2\n",
    "    elif next_direction == 'left':\n",
    "        action = 3\n",
    "    \n",
    "    return action\n",
    "\n",
    "#행동을 받고 다음 상태를 결정\n",
    "def get_s_next(s,a,Q,epsilon, pi_0):\n",
    "    direction = ['up','right','down','left']\n",
    "    next_direction = direction[a] #행동 a의 방향\n",
    "    \n",
    "    #행동으로 다음 상태 결정\n",
    "    if next_direction == 'up':\n",
    "        s_next = s - 3\n",
    "    elif next_direction == 'right':\n",
    "        s_next = s + 1\n",
    "    elif next_direction == 'down':\n",
    "        s_next = s + 3\n",
    "    elif next_direction == 'left':\n",
    "        s_next = s -1\n",
    "    \n",
    "    return s_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행동가치 함수 Q(s,a)를  Q-learning 알고리즘으로 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#행동가치 함수 Q를 학습하기 위해 Q-learning알고리즘의 적용\n",
    "\n",
    "def Q_learning(s, a, r, s_next, a_next, Q, eta, gamma):\n",
    "    \n",
    "    if s_next == 8: #목표 지점에 도착하면 다음 상태 존재 X\n",
    "        Q[s,a] = Q[s,a] + eta * (r-Q[s,a])\n",
    "    else :\n",
    "        Q[s,a] = Q[s,a] + eta * (r + gamma*np.nanmax(Q[s_next,:])- Q[s,a])\n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa로 미로찾기 구현\n",
    "가치반복 알고리즘에서는 정책경사 알고리즘과 달리, 매 시행이 업데이트의 단위가 아니라 매 단계(한 액션)가 단위가 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi):\n",
    "    s = 0 \n",
    "    a = a_next = get_action(s,Q, epsilon, pi) #첫번째 행동\n",
    "    s_a_history = [[0,np.nan]]\n",
    "    \n",
    "    while(1):\n",
    "        a = a_next\n",
    "        s_a_history[-1][1] = a\n",
    "        s_next = get_s_next(s,a,Q,epsilon,pi) #현재 상태에서 행동 함수를 통해(action이 정해지고) 다음 단계의 상태 구하고\n",
    "        s_a_history.append([s_next, np.nan])\n",
    "        \n",
    "        #다음 상태의 결과에 따라 보상을 부여하고 다음 행동을 계산\n",
    "        if s_next == 8:\n",
    "            r = 1\n",
    "            a_next = np.nan\n",
    "        else:\n",
    "            r = 0\n",
    "            a_next = get_action(s_next, Q, epsilon, pi)\n",
    "        \n",
    "        #보상을 받은 것을 고려하여 가치함수를 수정\n",
    "        Q = Q_learning(s, a, r, s_next, a_next, Q, eta, gamma)\n",
    "        \n",
    "        #종료 여부 판단\n",
    "        if s_next == 8: break\n",
    "        else: s = s_next \n",
    "            \n",
    "    return [s_a_history, Q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current episode num: 1\n",
      "0.00018185280861338082\n",
      "목표 지점까지 걸린 단계 수는 12 단계입니다.\n",
      "current episode num: 2\n",
      "7.805077733002985e-05\n",
      "목표 지점까지 걸린 단계 수는 6 단계입니다.\n",
      "current episode num: 3\n",
      "6.908579188824593e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 4\n",
      "6.377400164292446e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 5\n",
      "5.8860317914577465e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 6\n",
      "5.431585168058639e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 7\n",
      "5.011372171825901e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 8\n",
      "4.622892357908359e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 9\n",
      "4.263820625061143e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 10\n",
      "3.9319956151029345e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 11\n",
      "3.625408810270514e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 12\n",
      "3.342194294253531e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 13\n",
      "3.080619142881158e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 14\n",
      "2.839074412064324e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 15\n",
      "2.616066690575014e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 16\n",
      "2.41021018767551e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 17\n",
      "2.2202193253439972e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 18\n",
      "2.0449018070745062e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 19\n",
      "1.8831521359174985e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 20\n",
      "1.7339455554821193e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 21\n",
      "1.596332389230959e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 22\n",
      "1.4694327542863483e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 23\n",
      "1.3524316272439663e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 24\n",
      "1.2445742402222848e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 25\n",
      "1.1451617873303732e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 26\n",
      "1.0535474219031116e-05\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 27\n",
      "9.691325262628503e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 28\n",
      "8.91363236910081e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 29\n",
      "8.197272089005558e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 30\n",
      "7.537506036658925e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 31\n",
      "6.929952863554689e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 32\n",
      "6.370562184110362e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 33\n",
      "5.855590331727001e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 34\n",
      "5.381577818708294e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 35\n",
      "4.945328390459913e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 36\n",
      "4.54388956472318e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 37\n",
      "4.17453455825445e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 38\n",
      "3.834745504804893e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 39\n",
      "3.5221978782473684e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 40\n",
      "3.234746036473446e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 41\n",
      "2.970409811342556e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 42\n",
      "2.727362070853445e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 43\n",
      "2.5039171868135313e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 44\n",
      "2.2985203428360634e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 45\n",
      "2.109737628153141e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 46\n",
      "1.9362468570705005e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 47\n",
      "1.776829066213459e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 48\n",
      "1.6303606396039783e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 49\n",
      "1.4958060182701516e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 50\n",
      "1.3722109512004366e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 51\n",
      "1.2586962506722088e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 52\n",
      "1.1544520129858071e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 53\n",
      "1.0587322735178262e-06\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 54\n",
      "9.708500635641215e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 55\n",
      "8.90172839440595e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 56\n",
      "8.161182566412961e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 57\n",
      "7.481502651840444e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 58\n",
      "6.85775500164354e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 59\n",
      "6.285399488659138e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 60\n",
      "5.760258693365827e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 61\n",
      "5.278489482174464e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 62\n",
      "4.83655673511052e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 63\n",
      "4.43120911297612e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 64\n",
      "4.0594566697027545e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 65\n",
      "3.7185502166359186e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 66\n",
      "3.4059622611160023e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 67\n",
      "3.119369442750042e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 68\n",
      "2.856636307502214e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 69\n",
      "2.615800376304378e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 70\n",
      "2.3950583560861105e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 71\n",
      "2.1927534321619646e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 72\n",
      "2.007363565370568e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 73\n",
      "1.8374906896045928e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 74\n",
      "1.681850775314686e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 75\n",
      "1.5392646757206307e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 76\n",
      "1.4086496746834598e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 77\n",
      "1.289011722915845e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 78\n",
      "1.1794382903662637e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 79\n",
      "1.0790917648328957e-07\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 80\n",
      "9.872033979174688e-08\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 81\n",
      "9.030677128318843e-08\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 82\n",
      "8.260373851598501e-08\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 83\n",
      "7.555184966534512e-08\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 84\n",
      "6.909661987020144e-08\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 85\n",
      "6.318807110883284e-08\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 86\n",
      "5.7780364492998615e-08\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 87\n",
      "5.28314607617375e-08\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 88\n",
      "4.8302811306299986e-08\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 89\n",
      "4.415907028931798e-08\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 90\n",
      "4.0367832410126425e-08\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 91\n",
      "3.689939209738924e-08\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 92\n",
      "3.3726519577115255e-08\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 93\n",
      "3.082425881206774e-08\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 94\n",
      "2.8169738874872507e-08\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 95\n",
      "2.5742002196516012e-08\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 96\n",
      "2.3521846026497428e-08\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 97\n",
      "2.1491677548723942e-08\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 98\n",
      "1.9635379988613977e-08\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 99\n",
      "1.7938190488564487e-08\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n",
      "current episode num: 100\n",
      "1.6386587753380866e-08\n",
      "목표 지점까지 걸린 단계 수는 4 단계입니다.\n"
     ]
    }
   ],
   "source": [
    "# 함수들 전체 실행 코드\n",
    "\n",
    "eta = 0.1 #학습률\n",
    "gamma = 0.9 #시간할인률\n",
    "epsilon = 0.5 # 입실론 그리디 알고리즘에서 입실론 초기값\n",
    "v = np.nanmax(Q, axis = 1) #각 상태마다 가치의 최댓값 계산\n",
    "is_continue = True\n",
    "episode = 1\n",
    "V=[] #에피소드별로 상태가치를 저장\n",
    "V.append(np.nanmax(Q, axis=1)) #상태별로 행동가치의 최댓값을 계산\n",
    "\n",
    "while is_continue:\n",
    "    print(\"current episode num:\",episode)\n",
    "    \n",
    "    #한번 에피소드 마다 입실론 값 감소해서 랜덤의 비율 점점 줄이기\n",
    "    epsilon = epsilon/2\n",
    "    \n",
    "    #Sarsa알고리즘으로 미로를 빠져나온 후, 결과로 나온 행동 히스토리와 Q값을 변수에 저장\n",
    "    [s_a_history, Q] = goal_maze_ret_s_a_Q(Q, epsilon, eta, gamma, pi_0)\n",
    "    \n",
    "    #상태가치의 변화\n",
    "    new_v = np.nanmax(Q, axis = 1) #각 상태마다 행동가치의  최대값을 계산\n",
    "    print(np.sum(np.abs(new_v - v))) #상태가치 함수의 변화를 출력\n",
    "    v = new_v\n",
    "    V.append(v)\n",
    "    \n",
    "    print('목표 지점까지 걸린 단계 수는',len(s_a_history)-1, '단계입니다.')\n",
    "    \n",
    "    episode += 1\n",
    "    if episode >100: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
